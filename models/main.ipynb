{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pathlib\n",
    "import shutil\n",
    "import einops\n",
    "import typing\n",
    "import random\n",
    "import textwrap\n",
    "from typing import Any, Tuple\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_nlp\n",
    "#import tensorflow_text as tf_text\n",
    "from Constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spa.txt', '_about.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "  text = path.read_text(encoding = 'utf-8')\n",
    "\n",
    "  lines = text.splitlines()\n",
    "  pairs = [line.split('\\t') for line in lines]\n",
    "\n",
    "  context = np.array([context for target, context, _ in pairs])\n",
    "  target = np.array([target for target, context, _ in pairs])\n",
    "\n",
    "  return target, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected output: \n",
      " Un día, me desperté y vi que Dios me había puesto pelo en la cara. Me\n",
      "lo afeité. Al día siguiente, vi que Dios me lo había vuelto a poner en\n",
      "la cara, así que me lo afeité otra vez. Al tercer día, cuando vi que\n",
      "Dios me había puesto pelo en la cara de nuevo, decidí que Dios se\n",
      "saliera con la suya. Por eso tengo barba.\n",
      "\n",
      "Expected output: \n",
      " One day, I woke up to find that God had put hair on my face. I shaved\n",
      "it off. The next day, I found that God had put it back on my face, so\n",
      "I shaved it off again. On the third day, when I found that God had put\n",
      "hair back on my face again, I decided to let God have his way. That's\n",
      "why I have a beard.\n"
     ]
    }
   ],
   "source": [
    "target_raw, context_raw = load_data(data_file)\n",
    "print(\"Expected output: \\n\", '\\n'.join(textwrap.wrap(context_raw[-1])))\n",
    "print()\n",
    "print(\"Expected output: \\n\", '\\n'.join(textwrap.wrap(target_raw[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(context_raw)\n",
    "\n",
    "train = np.random.uniform(size = (len(target_raw), )) < 0.8\n",
    "\n",
    "raw_train_data = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context_raw[train], target_raw[train]))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "\n",
    "raw_val_data = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context_raw[~train], target_raw[~train]))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for string_example_context, string_example_target in raw_train_data.take(1):\n",
    "    print(string_example_context[:5])\n",
    "    print()\n",
    "    print(string_example_target[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = tf.constant(\"¿Todavía está en casa?\")\n",
    "\n",
    "print(sample_text.numpy())\n",
    "print(tf_text.normalize_utf8(sample_text, \"NFKD\").numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing steps include: \n",
    "- Splitting accented characters.\n",
    "- Keep spaces, alphabets and specific punctuations.\n",
    "- Add spaces around punctuations.\n",
    "- Strip whitespace.\n",
    "- Add start and end tokens around sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    \n",
    "    text = tf_text.normalize_utf8(text, \"NFKD\")\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "    text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "    text = tf.strings.strip(text)\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator = ' ')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_text.numpy().decode())\n",
    "print(text_preprocessing(sample_text).numpy().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_processor = keras.TextVectorization(\n",
    "    standardize = text_preprocessing,\n",
    "    max_tokens = MAX_VOCAB_SIZE,\n",
    "    ragged = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_processor.adapt(raw_train_data.map(lambda context, target: context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_processor = keras.TextVectorization(\n",
    "    standardize = text_preprocessing,\n",
    "    max_tokens = MAX_VOCAB_SIZE,\n",
    "    ragged = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_processor.adapt(raw_train_data.map(lambda context, target: target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tokens = context_processor(string_example_context)\n",
    "sample_tokens[:3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vocab = np.array(context_processor.get_vocabulary())\n",
    "tokens = context_vocab[sample_tokens[0].numpy()]\n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolormesh(sample_tokens.to_tensor())\n",
    "plt.title(\"Token ID's\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolormesh(sample_tokens.to_tensor() != 0)\n",
    "plt.title(\"Mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processor(context, target):\n",
    "    context = context_processor(context).to_tensor()\n",
    "    target = target_processor(target)\n",
    "    target_input = target[:, : -1].to_tensor()\n",
    "    target_output = target[:, 1 :].to_tensor()\n",
    "    \n",
    "    return (context, target_input), target_output\n",
    "\n",
    "train_set = raw_train_data.map(data_processor, tf.data.AUTOTUNE)\n",
    "val_set = raw_val_data.map(data_processor, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (sample_context_token, sample_target_input), sample_target_output in train_set.take(1):\n",
    "    print(sample_context_token[0, :10].numpy())\n",
    "    print()\n",
    "    print(sample_target_input[0, :10].numpy())\n",
    "    print(sample_target_output[0, :10].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, text_processor, units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.text_processor = text_processor\n",
    "        self.vocab_size = text_processor.vocabulary_size()\n",
    "        self.units = units\n",
    "        \n",
    "        # Converting tokens to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size,\n",
    "                                                   units\n",
    "                                                   mask_zero = True)\n",
    "        \n",
    "        # Processing vectors sequentially\n",
    "        self.rnn = tf.keras.layers.Bidirectional(merge_mode = \"sum\",\n",
    "                                                 layers = tf.keras.layers.GRU(units,\n",
    "                                                                              return_sequences = True,\n",
    "                                                                              recurrent_initializer = \"glorot_unitform\"))\n",
    "        \n",
    "    def call(self, x):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(x, 'batch s')\n",
    "        \n",
    "        # Embedding layers gets token for embedding vector\n",
    "        x = self.embedding(x)\n",
    "        shape_checker(x, \"batch s units\")\n",
    "        \n",
    "        # GRU processes embeddings\n",
    "        x = self.rnn(x)\n",
    "        shape_checker(x, \"batch s units\")\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def convert_input(self, texts):\n",
    "        texts = tf.convert_to_tensor(texts)\n",
    "        if len(texts.shape) == 0:\n",
    "            texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
    "        context = self.text_processor(texts).to_tensor()\n",
    "        context = self(context)\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode input sequence\n",
    "\n",
    "encoder = Encoder(context_processor, UNITS)\n",
    "sample_context = encoder(sample_context_token)\n",
    "\n",
    "print(f\"Context tokens, shape (batch, s): {sample_context_token.shape}\")\n",
    "print(f\"Encoder output, shape (batch, s): {sample_context.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = tf.keras.MultiHeadAttention(key_dims = units, num_heads = 1, **kwargs)\n",
    "        self.norm_layer = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        \n",
    "    def call(self, x, context):\n",
    "        shape_checker = ShapeChecker()\n",
    "        \n",
    "        shape_checker(x, \"batch t units\")\n",
    "        shape_checker(context, 'batch s units')\n",
    "        \n",
    "        attention_output, attention_score = self.multi_head_attention(query = x,\n",
    "                                                                      value = context,\n",
    "                                                                      return_attention_score = True)\n",
    "        \n",
    "        shape_checker(x, \"batch t units\")\n",
    "        shape_checker(attention_score, \"batch t s\")\n",
    "        \n",
    "        # Cache attention score for plotting later\n",
    "        attention_score = tf.reduce_mean(attention_score, axis = 1)\n",
    "        shape_checker(attention_score, \"batch t s\")\n",
    "        self.last_attention_weights = attention_score\n",
    "        \n",
    "        x = self.add([x, attention_output])\n",
    "        x = self.norm_layer(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "attention_layer = CrossAttention(UNITS)\n",
    "\n",
    "# Encoded tokens\n",
    "embedding = tf.keras.layers.Embedding(target_processor.vocabulary_size(),\n",
    "                                     output_dim = UNITS,\n",
    "                                     mask_zero = True)\n",
    "\n",
    "sample_embedded_target = embedding(sample_target_input)\n",
    "\n",
    "result = attention_layer(sample_embedded_target, sample_context)\n",
    "\n",
    "print(f\"Context sequence, shape (batch, s, units): {sample_context.shape}\")\n",
    "print(f\"Target sequence, shape (batch, t, units): {sample_embedded_target.shape}\")\n",
    "print(f\"Attention result, shape (batch, t, units): {result.shape}\")\n",
    "print(f\"Attention weights, shape (batch, t, s): {attention_layer.last_attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer.last_attention_weights[0].numpy().sum(axis =- 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights = attention_layer.last_attention_weights\n",
    "mask = (sample_context_token != 0).numpy()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolormesh(mask * attention_weights[:, 0, :])\n",
    "plt.title(\"Attention Weights\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolormesh(mask)\n",
    "plt.title(\"mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    @classmethod\n",
    "    \n",
    "    def add_method(cls, fun):\n",
    "        setattr(cls, fun.__name__, fun)\n",
    "        \n",
    "        return fun\n",
    "    \n",
    "    def __init__(self, text_processor, units):\n",
    "        super(Decoder, sellf).__init__()\n",
    "        self.text_processor = text_processor\n",
    "        self.vocab_size = text_processor.vocabulary_size()\n",
    "        self.word_to_id = tf.keras.layers.StringLookup(vocabulary = text_processor.get_vocabulary(),\n",
    "                                                       mask_token = '', \n",
    "                                                       oov_token = '[UNK]')\n",
    "        self.id_to_word = tf.keras.layers.StringLookup(vocabulary = text_processor.vocabulary_size(),\n",
    "                                                       mask_token = '',\n",
    "                                                       oov_token =  '[UNK]',\n",
    "                                                       invert = True)\n",
    "        self.start_token = self.word_to_id('[START]')\n",
    "        self.end_token = self.id_to_word('[END]')\n",
    "        \n",
    "        self.units = units\n",
    "        \n",
    "        # Embedding layer converts ids to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size,\n",
    "                                                   units,\n",
    "                                                   mask_zero = True)\n",
    "        \n",
    "        # RNN tracks generated sequences\n",
    "        self.RNN = tf.keras.layers.GRU(units,\n",
    "                                       return_sequences = True,\n",
    "                                       return_state = True,\n",
    "                                       recurrent_initializer = 'glorot_uniform')\n",
    "        \n",
    "        # RNN output becomes query for attention layer\n",
    "        self.attention = CrossAttention(units)\n",
    "        \n",
    "        # Fully connected layer produces logits for each output token\n",
    "        self.output_layer = tf.keras.layers.Dense(self.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "\n",
    "def call(self,\n",
    "         context, x,\n",
    "         state = None,\n",
    "         return_state = False):\n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(x, 'batch t')\n",
    "    shape_checker(context, 'batch s units')\n",
    "    \n",
    "    x = self.embedding(x)\n",
    "    shape_checker(x, 'batch t units')\n",
    "    \n",
    "    x, state = self.rnn(x, initial_state = state)\n",
    "    shape_checker(x, 'batch t units')\n",
    "    \n",
    "    x = self.attention(x, context)\n",
    "    self.last_attention_weights = self.attention.last_attention_weights\n",
    "    shape_checker(x, 'batch t units')\n",
    "    shape_checker(self.last_attention_weights, 'batch t s')\n",
    "    \n",
    "    logits = self.output_layer(x)\n",
    "    shape_checker(logits, 'batch t target_vocab_size')\n",
    "    \n",
    "    if return_state:\n",
    "        return logits, state\n",
    "    else:\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(target_processor, UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = decoder(context_sample, sample_target_input)\n",
    "\n",
    "print(f'Encoder output shape: (batch, s, units) {context_sample.shape}')\n",
    "print(f\"Input target tokens shape: (batch, t) {sample_target_input.shape}\")\n",
    "print(f\"logits shape: (batch, target_vocabulary_size) {logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "\n",
    "def get_initial_state(self, context):\n",
    "    batc_size = tf.shape(context)[0]\n",
    "    start_tokens = tf.fill([batch_size, 1], self.start_tokens)\n",
    "    done = tf.zeros([batch_size, 1], dtype = tf.bool)\n",
    "    embedded = self.emdedding(start_tokens)\n",
    "    \n",
    "    return start_tokens, done, self.rnn.get_initial_state(embedded)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "\n",
    "def tokens_to_text(self, tokens):\n",
    "    words = self.id_to_word(tokens)\n",
    "    result = tf.strings.reduce_join(words, axis = -1, separator = ' ')\n",
    "    result = tf.strings.regex_replace(result, '^ * \\[START\\] *', '')\n",
    "    result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "\n",
    "def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
    "    logits, state = self(context,\n",
    "                         next_token,\n",
    "                         state = state,\n",
    "                         return_state = True)\n",
    "    \n",
    "    if temperature == 0.0:\n",
    "        next_token = tf.argmax(logits, axis =- 1)\n",
    "    else:\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        next_token = tf.random.categorical(logits, num_samples = 1)\n",
    "        \n",
    "    # If sequence produces end token its done\n",
    "    done = done | (next_token == self.end_token)\n",
    "    # Once a sequence is done it only produces 0-padding\n",
    "    next_token = tf.where(done, tf.constant(0, dtype = tf.int64), next_token)\n",
    "    \n",
    "    return next_token, done, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation loop\n",
    "\n",
    "next_token, done, state = decoder.get_initial_state(context_sample)\n",
    "tokens = []\n",
    "\n",
    "for n in range(10):\n",
    "    next_token, done, state = decoder.get_next_token(context_sample,\n",
    "                                                     next_token,\n",
    "                                                     done,\n",
    "                                                     state,\n",
    "                                                     temperature = 1.0)\n",
    "    tokens.append(next_token)\n",
    "    \n",
    "tokens = tf.concat(tokens, axis =- 1)\n",
    "\n",
    "result = decoder.tokens_to_text(tokens)\n",
    "result[:3].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.keras.Model):\n",
    "    @classmethod\n",
    "    \n",
    "    def add_method(cls, fun):\n",
    "        setattr(cls, fun.__name__, fun)\n",
    "        \n",
    "        return fun\n",
    "    \n",
    "    def __init__(self, units, context_processor, target_processor):\n",
    "        super().__init__()\n",
    "        # Add Encoder and Decoder\n",
    "        encoder = Encoder(context_processor, units)\n",
    "        decoder = Decoder(target_processor, units)\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        context, x = inputs\n",
    "        context = self.encoder(context)\n",
    "        logits = self.decoder(context, x)\n",
    "        \n",
    "        # Delete mask\n",
    "        try:\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Translator(UNITS, context_processor, target_processor)\n",
    "logits = model((sample_context_token, sample_target_input))\n",
    "\n",
    "print(f\"Context tokens shape: {sample_context_token.shape}\")\n",
    "print(f\"Target tokens shape: {sample_target_input}\")\n",
    "print(f\"logits shape: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(y_true, y_predicted):\n",
    "    loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True,\n",
    "                                                              reduction = 'none')\n",
    "    loss = loss_func(y_true, y_predicted)\n",
    "    \n",
    "    mask = tf.cast(y_true != 0, loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_accuracy(y_true, y_predicted):\n",
    "    y_predicted = tf.argmax(y_predicted, axis =- 1)\n",
    "    y_predicted = tf.cast(y_predicted, y_true.dtype)\n",
    "    \n",
    "    match = tf.cast(y_true == y_predicted, tf.float32)\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "    \n",
    "    return tf.reduce_sum(match) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "              loss = masked_loss\n",
    "              metrics = [masked_accuracy, masked_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 1.0 * target_processor.vocabulary_size()\n",
    "\n",
    "{\"expected_loss\" : tf.math.log(vocabulary_size).numpy(),\n",
    " \"expected_accuracy\" : 1 / vocabulary_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(val_set,\n",
    "               steps = 20,\n",
    "               return_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_set.repeat(),\n",
    "                    validation_data = val_set,\n",
    "                    #steps_per_epoch = 100,\n",
    "                    validation_steps = 20,\n",
    "                    callbacks = [keras.callbacks.EarlyStopping(patience = 3)],\n",
    "                    epochs = 100\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = history['val_loss']\n",
    "loss = history['loss']\n",
    "\n",
    "val_accuracy = history[\"val_masked_accuracy\"]\n",
    "accuracy = history[\"masked_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracy, label = \"Loss\")\n",
    "plt.plot(val_accuracy, label = \"Validation Loss\")\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"token\")\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss, label = \"Loss\")\n",
    "plt.plot(val_loss, label = \"Validation Loss\")\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"token\")\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text to text translation\n",
    "- Process input text\n",
    "- Generate next token\n",
    "- Store generated tokens\n",
    "- Stack tokens and attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "@Translator.add_method\n",
    "\n",
    "def translate(self, texts, *, max_length = 50, temperature = 0.0):\n",
    "    context = self.encoder.convert_input(texts)\n",
    "    batch_size = tf.shape(texts)[0]\n",
    "    \n",
    "    tokens = []\n",
    "    attention_weights = []\n",
    "    next_token, done, state = self.decoder.get_initial_state(context)\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        next_token, done, state = self.decoder.get_next_token(context,\n",
    "                                                              next_token,\n",
    "                                                              done, \n",
    "                                                              state,\n",
    "                                                              temperature)\n",
    "        \n",
    "        tokens.append(next_token)\n",
    "        attention_weights.append(self.decoder.last_attention_weights)\n",
    "        \n",
    "        if tf.executin_eagerly() and tf.reduce_all(done):\n",
    "            break\n",
    "        \n",
    "    tokens = tf.concat(tokens, axis =- 1)\n",
    "    self.last_attention_weights = tf.concat(attention_weights, axis = 1)    \n",
    "    result = self.decoder.tokens_to_text(tokens)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Translator.add_method\n",
    "\n",
    "def attention_plot(self, text, **kwargs):\n",
    "    asser isinstance(text, str)\n",
    "    output = self.translate{[text], **kwargs}\n",
    "    output = output[0].numpy().decode()\n",
    "    \n",
    "    attention = self.last_attention_weights[0]\n",
    "    \n",
    "    context = text_processing(text)\n",
    "    context = context.numpy().decode().split()\n",
    "    \n",
    "    output = text_processing(output)\n",
    "    output = output.numpy().decode().split()[1:]\n",
    "    \n",
    "    fig = plt.figure(figsize = (12, 8))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap = 'viridis', vmin = 0.0)\n",
    "    \n",
    "    fontdict = {\"fontsize\" : 13}\n",
    "    \n",
    "    ax.set_xticklabels([''] + context , fontdict = fontdict, rotation = 90)\n",
    "    ax.set_yticklabels([''] + output, fontdict = fontdict)\n",
    "    \n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    ax.set_xlabel(\"Input text\")\n",
    "    ax.set_ylabel(\"Output text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.attention_plot(\"¿Ningun esta en casa?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model.attention_plot(\"Esto es mi vida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.attention_plot(\"Tratar de descubrir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected output: \n",
      " One day, I woke up to find that God had put hair on my face. I shaved\n",
      "it off. The next day, I found that God had put it back on my face, so\n",
      "I shaved it off again. On the third day, when I found that God had put\n",
      "hair back on my face again, I decided to let God have his way. That's\n",
      "why I have a beard.\n"
     ]
    }
   ],
   "source": [
    "long_text = context_raw[-1]\n",
    "\n",
    "print(\"Expected output: \\n\", '\\n'.join(textwrap.wrap(target_raw[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.attention_plot(long_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\"Hace mucho calor aqui.\",                # Its very hot here.\n",
    "           \"Quiero aprender a hablar espanol.\",     # I want to learn to speak spanish.\n",
    "           \"El cuatro esta sucio.\"]                   # The room is dirty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for sample in samples:\n",
    "    print(model.translate([sample])[0].numpy().decode())\n",
    "    \n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save and export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Export(tf.Module):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    @tf.function(input_signature = [tf.TensorSpec(dtype = tf.string, shape = [None])])\n",
    "    def translate(self, inputs):\n",
    "        return self.model.translate(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator_model = Export(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = translator_model.translate(tf.constant(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result = translator_model.translate(tf.constant(samples))\n",
    "\n",
    "print(result[0].numpy().decode())\n",
    "print(result[1].numpy().decode())\n",
    "print(result[2].numpy().decode())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tf.saved_model.save(translator_model,\n",
    "                    'translator',\n",
    "                    signatures = {'serving_default' : translator_model.translate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "loaded_model = tf.saved_model.load('translator')\n",
    "_ = loaded_model.translate(tf.constant(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result = loaded_model.translate(tf.constant(samples))\n",
    "\n",
    "print(result[0].numpy().decode())\n",
    "print(result[1].numpy().decode())\n",
    "print(result[2].numpy().decode())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using a dynamic loop\n",
    "\n",
    "- This is faster than the eqger execution implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Translator.add_method\n",
    "\n",
    "def translate(self, texts, *, max_length = 500, temperature = tf.constant(0.0)):\n",
    "    shape_checker = ShapeChecker()\n",
    "    context = self.encoder.convert_input(texts)\n",
    "    batch_size = tf.shape(context)[0]\n",
    "    shape_checker(context, 'batch s units')\n",
    "    \n",
    "    next_token, done, state = self.decoder.get_initial_state(context)\n",
    "    \n",
    "    tokens = tf.TensorArray(tf.int64, size = 1, dynamic_size = True)\n",
    "    \n",
    "    for t in tf.range(max_length):\n",
    "        next_token, done, state = self.decoder.get_next_token(context, \n",
    "                                                              next_token,\n",
    "                                                              done,\n",
    "                                                              state,\n",
    "                                                              temperature)\n",
    "        shape_checker(next_token, 'batch t1')\n",
    "        \n",
    "        tokens = tokens.write(t, next_token)\n",
    "        \n",
    "        if tf.reduce_all(done):\n",
    "            break\n",
    "        \n",
    "    tokens = tokens.stack()\n",
    "    shape_checker(tokens, 't batch t1')\n",
    "    tokens = einops.rearrange(tokens, 't batch 1 -> batch t')\n",
    "    shape_checker(tokens, 'batch t')\n",
    "    \n",
    "    text = self.decoder.tokens_to_text(tokens)\n",
    "    shape_checker(text, 'batch t')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result = model.translate(samples)\n",
    "\n",
    "print(result[0].numpy().decode())\n",
    "print(result[1].numpy().decode())\n",
    "print(result[2].numpy().decode())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Export(tf.Module):\n",
    "    \n",
    "    def __iniit__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    @tf.function(input_signature = [tf.TensorSpec(dtype = tf.string, shape = [None])])\n",
    "    def translate(self, inputs):\n",
    "        \n",
    "        return self.model.translate(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn_model = Export(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "_ = dyn_model.translate(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result = dyn_model.translate(samples)\n",
    "\n",
    "print(result[0].numpy().decode())\n",
    "print(result[1].numpy().decode())\n",
    "print(result[2].numpy().decode())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tf.saved_model.save(dyn_model,\n",
    "                    'dynamic_translator',\n",
    "                    signatures = {'serving_default' : dyn_model.translate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "load_dyn_model = tf.saved_model.load('dynamic_translator')\n",
    "_ = load_dyn_model.translate(tf.constant(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result = load_dyn_model.translate(tf.constant(samples))\n",
    "\n",
    "print(result[0].numpy().decode())\n",
    "print(result[1].numpy().decode())\n",
    "print(result[2].numpy().decode())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
